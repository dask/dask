Query planning with Expression system
=====================================

.. note::

    This document is intended for Dask developers and contributors. It is not
    intended for end-users.
    For a high level user guide, see :doc:`/dataframe-optimizer`.

.. currentmodule:: dask.dataframe

The expression system was initially implemented in the exclusive context of DataFrames (see `dask-expr <https://github.com/dask/dask-expr>`_). Very early versions where building on `matchpy <https://github.com/HPAC/matchpy>`_ which was soon rejected in favor of a custom built system with simpler optimization steps.

Expr objects
------------

The expression system is built around the `Expr` class. This class is used to represent a computation that can be performed on a Dask DataFrame. The `Expr` class is designed to be subclassed, and each subclass represents a specific type of computation. For example, there are subclasses for arithmetic operations, logical operations, and so on.

Construction
~~~~~~~~~~~~

Before discussing the optimization itself, we'll discuss the core ``Expr`` class itself (see :class:`~dask.dataframe._expr.Expr`; Note, there are multiple `Expr` classes with the same name).

A very early design decision was to prohibit any use of custom initializers in
any subclass or even the base class. A concern was that these objects would be
create and recreated many times and custom logic in the initializer would slow
things down.

To mimic a dataclass like interface, the types can defined two attributes that
define the names of provided arguments and possible default values. There is
only very minimal input validation performed and it is possible to provide an
arbitrary number of arguments to the constructor. All arguments will be stored
under the attribute `operands`.

.. code::

    >>> class MyExpr(Expr):
    >>>     _parameters = ["param1", "param2"]
    >>>     _defaults = {"param2": None}


    >>> class MyExpr(Expr):
    >>>     _parameters = ["param1", "param2"]
    >>>     _defaults = {"param2": None}

    >>> expr = MyExpr(1, 2, 3)
    >>> expr.param1
    1

    >>> expr.param2
    2

    >>> expr.operands
    [1, 2, 3]

Names and tokens
~~~~~~~~~~~~~~~~

The system heavily relies on tokenization since every expression defines a unique name that is built by a prefix, typically the name of the class or a derivative thereof, and a unique token that is generated by tokenizing the operands. The tokenization is done by the `tokenize` function in the `dask.base` module. This function generates a unique hash for the operands and returns it as a string.

This tokenization step is crucial since the name is used to deduplicate nodes in the expression tree, determine whether an optimization step changed anything and in many cases it is even used to enforce a singleton pattern such that there exists only one instance with a given name/token (Iff the expression is subclassed by :class:`~dask.dataframe._expr.SingletonExpr` which is the case for all DataFrame expressions)

As critical as this is, it is also a big problem for a few reasons:

- Tokenization is relatively slow due to the recursive traversal of the objects
  and due to the Dask dispatch mechanism.

- Many objects do not implement a custom `__dask_tokenize__` method or register
  one via the registration mechanism. This causes us to fall back to pickle or
  cloudpickle and hash their dump (which is slow). This also isn't absolutely
  reliable nor deterministic (partially because pickle is mutating some low
  level CPython state of the objects such that two subsequent pickles can look
  differently).

- Tokenization across interpreters (or hosts) complicates things. Especially
  cloudpickled objects can look differently and we cannot rely on any kind of
  caching. Therefore, relying on tokens across interpreters to validate
  uniqueness is not possible. This has to be taken into account for client to
  scheduler graph submission because (user/client visible) keys have to be
  locked in before things go to the scheduler (more on this later)

To bypass most of these problems the classes are computing their name and their
token upon construction, cache them on the instance and serialize this
pre-computed token such that the name is pickle roundtrip stable. This token is
accessible under `_determ_token` or `deterministic_token`.

Caching and Singletons
~~~~~~~~~~~~~~~~~~~~~~

Even though one of the primary goals of not allowing initializers was to keep
expressions as stateless as possible and avoid expensive computation of that
state. The reality is that expressions are very stateful and we are performing
many of these operations all over the place. Instead of having them in a
centralized constructor, they are computed on demand and cached on the instance.
This is done by defining expensive attributes as a `property`` using the
`functools.cached_property` decorator. While delaying some computations can be
helpful, in practice this makes reasoning about the code much harder. It is
difficult to understand when certain properties are or were calculated.

Further, some of the cached attributes are sensitive to the tokens / names of
the expression classes. Therefore, most expressions are serializing the
functools cache unless the expression type opts out of this behavior by setting
`_pickle_functools_cache`

During optimization, expressions are recreated over and over again which under
ordinary circumstances would cause us to loose the cached properties. Therefore,
most expressions are inheriting from
:class:`~dask.dataframe._expr.SingletonExpr` which computes the unique name of a
new instance in their constructor (:meth:`~dask._expr.Expr.__new__`) and returns an already existing,
cache populated instances with the same name. This effectively, implements a
singleton pattern for those expressions. It is therefore very important to not
inplace mutate any state!

Optimization procedure
----------------------

If an expression is provided to another expression as an argument / operand it
is considered a dependency of this expression. Therefore, expressions naively
span a directed, rooted tree structure with a single root node. The root node is
the last expression that was applied.
Since expressions are deduplicated by their name, the structure is however
typically not a tree but a more general directed acyclic graph (DAG) which is an
important property for the optimization passes.

At the time of writing, the optimizer performs five optimization steps

- simplify
- rewrite/tune
- lower
- simplify (again)
- fuse

Simplify
~~~~~~~~

Simplify optimizations are typically swapping the order of expressions. This is for example where `Projection` or `Filter` pushdown is implemented. By repeatedly doing this, we can push a filter down the entire expression graph.

There are a couple of constraints for the kind of optimizations that should be performed here. None of them are confirmed at runtime.

- `npartitions` should not increase
- No computation with side effects should be triggered (e.g. compute divisions)

Rewrite / tune
~~~~~~~~~~~~~~

Tuning is an optimization step that implements rather subjective performance
optimizations that typically relate to the cardinality of the computation graph.
An example is the `FusedIO` optimization where multiple partitions are fused in
place triggered by column projection. Likewise, this is a step where we are
trying to make a good guess on a `split_out`. This step is not altering the
nature of the expression but is rather modifying subtle performance relevant
properties of the same expression.

Lowering
~~~~~~~~

Up until this point we were typically dealing with rather abstract operations
that tell us what they want to do but not how. An example is `Merge` which tells
us to merge two (or more) DataFrames but not how. For example, if we know that
both dataframes where previously partitioned/shuffled on the same column, this
merge can be performed trivially since the partitions are already aligned and
would therefore lower to a more concrete expression. In this case,
`BlockwiseMerge`. If no index information is known, it may otherwise fall back
to a general `HashJoinP2P`, etc.
In traditional query planning this (almost) marks the transition from a logical
to a physical plan.
We call again a simplify step on the output of this since some of the lowered
classes can still simplify themselves.

Fuse
~~~~

Finally, we are fusing blockwise tasks. Basically linear chains of tasks are
smashed into a single task to reduce scheduling overhead.


Walking the graph during optimization
-------------------------------------

To show how the optimizer walks the expression graph we will take :meth:`~dask.dataframe._expr.Expr.simplify` as an example. Other optimizer steps work similarly.

During one of the optimizer stages the optimizer walks repeatedly over the
expression graph until no further optimizations can be made. Walking the graph
once is performed in :meth:`~dask.dataframe._expr.Expr.simplify_once` which
takes the root expression (the instance this is called on) and starts by calling
:meth:`~dask.dataframe._expr.Expr._simplify_down` on itself.

The suffix `down` is an indication on the direction of the traversal. In this
context, `down` means that the optimizer step only has access to the instance
itself and whatever dependencies it can access via the `operands` attribute. The
method is expected to return either a new expression if something changed or
`None` if no simplification was possible.
If an expression is returned, we are verifying that the returned expression
indeed has a new name since the same name would also imply that no optimization
was possible (see also tokenization above).

Next, the optimizer calls :meth:`~dask.dataframe._expr.Expr._simplify_up` on
every dependency of the expression. The dependency is provided with the parent
(the current expression) and a mapping of dependents of the entire expression
graph structure. This is where optimizations can be performed that need a
non-local information. For example, do any of my siblings share common parents?
If so, do...
This `simplify_up` is expected to return a new expression that should replace
the _parent_, i.e. it is not replacing itself!

Generally, every `simplify_down` call can be expressed as a `simplify_up` but considering the simpler interface of `_down` optimization it is encouraged to start with a downwards optimization and only generalize if necessary.

Finally, the optimizer calls `simplify_once` on every dependency. This is where
the graph traversal takes place since every node is reachable by walking the
dependencies recursively.

The careful reader might've noticed that this pattern might actually never
converge. Indeed, it is possible to define optimization patterns that cause
basically infinite recursion errors. There are some safeguards in place by
checking for already observed expressions (again by name). Similarly, some of
these patterns where causing exponential runtimes even in a single pass, see
`dask-expr#835 <https://github.com/dask/dask-expr/issues/835>`_ which required
the introduction of memoization in most optimizer steps.
All of this does not protect from weird corner cases and practically infinite
optimization runtimes popped up over and over again (albeit increasingly more
rarely).


Expressions as the scheduler / client interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To transmit a computation graph to the scheduler we are submitting the
expression instead of the low level task graph. This is typically cheaper but
comes with some additional complexity.
At the time of writing, the ``distributed.Client`` requires knowledge of final output task names **before** the graph is submitted to the scheduler. This is a problem since tokenization is not deterministic across interpreters and optimization is built such that key names are changing. Therefore, optimization is performed before submission. This locks in the key names and causes all caches to be populated.
A further caveat are expressions like the `ReadParquet` expression which require
non-trivial IO to collect partitions statistics. This typically requires a
Client / full scheduler interface and we do not want to perform this computation
during graph materialization on scheduler. This currently happens during the
lowering optimization step.

Legacy HighLevelGraph support
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

HighLevelGraphs (HLG) are a legacy graph representation that are still used for
Arrays, Bags and most Delayed objects. They have a long track record of being
problematic in terms of serialization. While their initial design goal was to
defer low level graph materialization until after the HLG is submitted to the
scheduler, there are way too many code paths that trigger "accidental
materialization", i.e. they convert the graph to a low level representation
behind the scenes. Most of the time this is happening during "low level
optimization" which is why it is crucial to move this step to the scheduler when
transmitting HLGs. However, the kind of optimizer that is supposed to be used is
unknown to the HLG. Similarly, the kind of postcompute (i.e. how to concatenate
multiple partitions to a single result) us unknown to the HLG. Both is dependent
on the _collection_ being used. Therefore, HLGs have to be "enriched" with this
kind of information to be able to implement a full expression interface. For
this purpose, there is the `HLGExpr` class which wraps a full HLG and guarantees
the delay of materialization until the scheduler is calling `__dask_graph__`
explicitly which in turn calls the low level optimization, i.e. low level
optimization is not part of the materialization step.
